{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d8a0e8-e15c-4cc0-bb82-16f50d9d4382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "# Load the dataset\n",
    "df = pd.read_csv('mail_data.csv')\n",
    "# Describe the dataset\n",
    "print(f\"Number of instances (emails): {df.shape[0]}\")\n",
    "print(f\"Attributes: {df.columns.tolist()}\")\n",
    "print(df.isnull().sum())\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'\\W+', ' ', text)   # Remove special characters and digits\n",
    "    text = text.lower()                # Convert text to lowercase\n",
    "    words = text.split()               # Tokenization\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]  # Remove stop words and lemmatize\n",
    "    return ' '.join(words)\n",
    "df['processed_text'] = df['Message'].apply(preprocess_text)\n",
    "df['label'] = df['Category'].apply(lambda x: 1 if x == 'spam' else 0)\n",
    "# Display the first few rows of the processed dataset\n",
    "print(df.head())\n",
    "# Feature Extraction\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df['processed_text'])\n",
    "y = df['label']\n",
    "print(X.shape)\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Define hyperparameter grids for each classifier\n",
    "param_grids = {\n",
    "    \"Logistic Regression\": {\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'solver': ['newton-cg', 'lbfgs', 'liblinear']\n",
    "    },\n",
    "    \"Naive Bayes\": {\n",
    "        'alpha': [0.01, 0.1, 1, 10]\n",
    "    },\n",
    "    \"Support Vector Machine\": {\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "    },\n",
    "    \"Decision Tree\": {\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'max_depth': [None, 10, 20, 30]\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        'n_estimators': [50, 100],\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'max_depth': [None, 10, 20]\n",
    "    },\n",
    "    \"k-Nearest Neighbors\": {\n",
    "        'n_neighbors': [3, 5, 7],\n",
    "        'weights': ['uniform', 'distance']\n",
    "    },\n",
    "    \"Gradient Boosting\": {\n",
    "        'n_estimators': [50, 100],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'max_depth': [3, 4]\n",
    "    }\n",
    "}\n",
    "# Define a dictionary of classifiers\n",
    "classifiers = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"Support Vector Machine\": SVC(probability=True),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"k-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier()\n",
    "}\n",
    "# Evaluate and optimize each classifier\n",
    "best_estimators = {}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"\\n{name} - Before Optimization\")\n",
    "    \n",
    "    # Evaluate before optimization\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_prob = clf.predict_proba(X_test)[:, 1] if hasattr(clf, 'predict_proba') else [0]*len(y_pred)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"AUC-ROC: {roc_auc_score(y_test, y_prob):.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    # Optimize with GridSearchCV\n",
    "    print(f\"\\n{name} - After Optimization\")\n",
    "    param_grid = param_grids[name]\n",
    "    grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    print(\"Best cross-validation score:\", grid_search.best_score_)\n",
    "    \n",
    "    best_clf = grid_search.best_estimator_\n",
    "    best_estimators[name] = best_clf\n",
    "    \n",
    "    # Evaluate after optimization\n",
    "    best_clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = best_clf.predict(X_test)\n",
    "    y_prob = best_clf.predict_proba(X_test)[:, 1] if hasattr(best_clf, 'predict_proba') else [0]*len(y_pred)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"AUC-ROC: {roc_auc_score(y_test, y_prob):.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "# Initialize a dictionary to store performance metrics\n",
    "metrics = {\n",
    "    \"Classifier\": [],\n",
    "    \"Accuracy\": []\n",
    "}\n",
    "import matplotlib.pyplot as plt\n",
    "# Evaluate each classifier before optimization\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Store the metrics\n",
    "    metrics[\"Classifier\"].append(name)\n",
    "    metrics[\"Accuracy\"].append(accuracy)\n",
    "\n",
    "# Create a DataFrame from the metrics dictionary\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "\n",
    "# Plot the accuracy metrics\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(metrics_df[\"Classifier\"], metrics_df[\"Accuracy\"], color='b')\n",
    "plt.xlabel('Classifiers')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Classifier Accuracy Before Optimization')\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "# Add the values on top of the bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.annotate(f'{height:.4f}',\n",
    "                 xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                 xytext=(0, 3),  # 3 points vertical offset\n",
    "                 textcoords=\"offset points\",\n",
    "                 ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Initialize a dictionary to store performance metrics\n",
    "metrics = {\n",
    "    \"Classifier\": [],\n",
    "    \"Before Optimization\": [],\n",
    "    \"After Optimization\": []\n",
    "}\n",
    "\n",
    "# Evaluate and optimize each classifier\n",
    "best_estimators = {}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    # Evaluate before optimization\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy_before = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Store the metrics before optimization\n",
    "    metrics[\"Classifier\"].append(name)\n",
    "    metrics[\"Before Optimization\"].append(accuracy_before)\n",
    "    \n",
    "    # Optimize with GridSearchCV\n",
    "    param_grid = param_grids[name]\n",
    "    grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_clf = grid_search.best_estimator_\n",
    "    best_estimators[name] = best_clf\n",
    "    \n",
    "    # Evaluate after optimization\n",
    "    y_pred = best_clf.predict(X_test)\n",
    "    accuracy_after = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Store the metrics after optimization\n",
    "    metrics[\"After Optimization\"].append(accuracy_after)\n",
    "\n",
    "# Create a DataFrame from the metrics dictionary\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "\n",
    "# Plot the accuracy metrics for each classifier\n",
    "for classifier in metrics_df[\"Classifier\"]:\n",
    "    before = metrics_df.loc[metrics_df[\"Classifier\"] == classifier, \"Before Optimization\"].values[0]\n",
    "    after = metrics_df.loc[metrics_df[\"Classifier\"] == classifier, \"After Optimization\"].values[0]\n",
    "    \n",
    "    plt.figure(figsize=(6, 4))\n",
    "    bars = plt.bar([\"Before Optimization\", \"After Optimization\"], [before, after], color=['b', 'g'])\n",
    "    plt.xlabel('Optimization Status')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f'{classifier} Accuracy Before and After Optimization')\n",
    "    \n",
    "    # Add the values on top of the bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.annotate(f'{height:.4f}',\n",
    "                     xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                     xytext=(0, 3),  # 3 points vertical offset\n",
    "                     textcoords=\"offset points\",\n",
    "                     ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "!pip install joblib\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df['processed_text'])\n",
    "y = df['label']\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "random_forest_model.fit(X, y)\n",
    "import joblib\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(random_forest_model, 'random_forest_model.pkl')\n",
    "# Save the vectorizer\n",
    "joblib.dump(vectorizer, 'vectorizer.pkl')\n",
    "# Function to check if the email is spam\n",
    "def check_spam(email_text):\n",
    "    processed_input_mail = preprocess_text(email_text)\n",
    "    input_data_features = vectorizer.transform([processed_input_mail])\n",
    "\n",
    "    predict = random_forest_model.predict(input_data_features)\n",
    "\n",
    "    return predict[0] == 1\n",
    "\n",
    "# User Input for Spam Detection using RandomForestClassifier\n",
    "input_mail = input(\"Enter your mail content: \")\n",
    "is_spam = check_spam(input_mail)\n",
    "\n",
    "if is_spam:\n",
    "    print(\"It is a spam mail\")\n",
    "else:\n",
    "    print(\"It is not a spam mail\")\n",
    "pip install termcolor\n",
    "import imaplib\n",
    "import email\n",
    "from email.header import decode_header\n",
    "import getpass\n",
    "import joblib\n",
    "import re\n",
    "from termcolor import colored\n",
    "\n",
    "# Load the pre-trained Random Forest model and the vectorizer\n",
    "try:\n",
    "    random_forest_model = joblib.load('random_forest_model.pkl')\n",
    "    vectorizer = joblib.load('vectorizer.pkl')\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Model or vectorizer file not found: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Function to preprocess the email text for spam detection\n",
    "def preprocess_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # Remove non-letters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Function to check if the email is spam\n",
    "def check_spam(email_text):\n",
    "    processed_input_mail = preprocess_text(email_text)\n",
    "    input_data_features = vectorizer.transform([processed_input_mail])\n",
    "\n",
    "    predict = random_forest_model.predict(input_data_features)\n",
    "\n",
    "    return predict[0] == 1\n",
    "\n",
    "# Function to fetch emails from Gmail using IMAP\n",
    "def fetch_emails(username, password, count=5):\n",
    "    try:\n",
    "        # Connect to Gmail's IMAP server\n",
    "        mail = imaplib.IMAP4_SSL('imap.gmail.com')\n",
    "        mail.login(username, password)\n",
    "        \n",
    "        # Select the inbox\n",
    "        mail.select('inbox')\n",
    "        \n",
    "        # Search for all emails and fetch the most recent ones\n",
    "        status, data = mail.search(None, 'ALL')\n",
    "        mail_ids = data[0].split()\n",
    "\n",
    "        # Fetch the most recent 'count' emails\n",
    "        for i in range(-1, -(count+1), -1):\n",
    "            if abs(i) <= len(mail_ids):\n",
    "                status, data = mail.fetch(mail_ids[i], '(RFC822)')\n",
    "                for response_part in data:\n",
    "                    if isinstance(response_part, tuple):\n",
    "                        msg = email.message_from_bytes(response_part[1])\n",
    "                        \n",
    "                        # Decode the email subject\n",
    "                        subject, encoding = decode_header(msg[\"Subject\"])[0]\n",
    "                        if isinstance(subject, bytes):\n",
    "                            subject = subject.decode(encoding if encoding else 'utf-8')\n",
    "                        \n",
    "                        print(f\"Email {abs(i)}: Subject - {subject}\")\n",
    "\n",
    "                        # Fetch the email body\n",
    "                        body = \"\"\n",
    "                        if msg.is_multipart():\n",
    "                            for part in msg.walk():\n",
    "                                content_type = part.get_content_type()\n",
    "                                content_disposition = str(part.get(\"Content-Disposition\"))\n",
    "\n",
    "                                if \"attachment\" not in content_disposition:\n",
    "                                    # Get the email body\n",
    "                                    payload = part.get_payload(decode=True)\n",
    "                                    if content_type == \"text/plain\" and payload:\n",
    "                                        body += payload.decode('utf-8') + \"\\n\"\n",
    "                        else:\n",
    "                            # If the email is not multipart\n",
    "                            payload = msg.get_payload(decode=True)\n",
    "                            if payload:\n",
    "                                body = payload.decode('utf-8')\n",
    "                        \n",
    "                        print(f\"Body: {body.strip()}\")\n",
    "                        is_spam = check_spam(body)\n",
    "                        spam_status = \"Spam\" if is_spam else \"Not Spam\"\n",
    "                        color = \"red\" if is_spam else \"green\"\n",
    "                        print(colored(f\"Spam Status: {spam_status}\", color))\n",
    "            else:\n",
    "                break\n",
    "        mail.logout()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching emails: {e}\")\n",
    "def get_credentials():\n",
    "    email_address = input(\"Enter your Gmail email address: \")\n",
    "    password = getpass.getpass(\"Enter your password: \")\n",
    "    return email_address, password\n",
    "def main():\n",
    "    email_address, password = get_credentials()\n",
    "    fetch_emails(email_address, password, count=5)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
